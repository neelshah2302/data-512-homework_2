{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HW2 - Data Ingestion and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json, time, urllib.parse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get US cities and related articles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_wiki_df = pd.read_csv('us_cities_by_state_SEPT.2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Info Extraction using API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Page info for all articles using API call to Wikipedia. Using the example code to do the API call and set constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to get page info for each article title. We will pass article title to this function and it will store the information related to the article including the revision ID into a dictionary for all 22,157 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition\n",
    "def get_page_info(df):\n",
    "    article_title = df['page_title']\n",
    "    try:\n",
    "        info = request_pageinfo_per_article(article_title)\n",
    "        page_dict = info['query']['pages']\n",
    "        all_page_info_dict.update(page_dict)\n",
    "    except:\n",
    "        no_page_info_list.append(article_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty dictionary and then append all the generated dict outputs from the web pull into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call\n",
    "all_page_info_dict = dict()\n",
    "no_page_info_list = []\n",
    "x = cities_wiki_df.iloc.apply(get_page_info,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the final dictionary with all the dicts into a dataframe and save it for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info_df = pd.DataFrame(all_page_info_dict).T.reset_index(drop=True)\n",
    "page_info_df.to_csv('page_info_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORES Score extraction using API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a final page info df which has article title and corresponding revision ID which will be used for ORES score pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info_df_final = page_info_df[['pageid','title','lastrevid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Quality Scores for all articles using API call to ORES. Using the example code to do the API call and set constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<nshah23@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting username and Access Token which I created using Wikimedia account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \"Nshah23\"\n",
    "ACCESS_TOKEN = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIwMjExZWE0YzgxYjI1OTBlYjAyMzE4MzkwOGQ5NDRhYSIsImp0aSI6IjA0YjIxZGZiMjkzZWVjYmZkNmUxNmY2OWZkMTQ5YjYzODRiMzdmNTRiYzJmNTNlMWQxN2IwYzFhZDkzYzQwNjE0OGJiNTc4MmZiM2VlNmQ2IiwiaWF0IjoxNjk3NDk5MzY4LjI0NzM0NywibmJmIjoxNjk3NDk5MzY4LjI0NzM1LCJleHAiOjMzMjU0NDA4MTY4LjI0NDc3NCwic3ViIjoiNzQwMjE2MTIiLCJpc3MiOiJodHRwczovL21ldGEud2lraW1lZGlhLm9yZyIsInJhdGVsaW1pdCI6eyJyZXF1ZXN0c19wZXJfdW5pdCI6NTAwMCwidW5pdCI6IkhPVVIifSwic2NvcGVzIjpbImJhc2ljIl19.CB6-OrWFJZ7DMAyv_Iktz7UKL_e2Wz_ZeFY6lZQUREpgntObptjYq-GKgKVfKXfEE19HZTg_RCCe4rbQ5mFDcCP9Jg5TFXQWOE8ot5QCi4dnOPjlV0DCWNmJQQRU7ulwvl1Bqzmt33REU9n9FszvZ65vaRKfHq_leB7SB-Yldr9qUfsRqJ3nUDLPXer6NaFwR_YUPP-9gh225SgMgjW-_6n-vDYBtrw-3WL4PcXbZImcMA-J0_0QNUA-LyfwqodmCSIXexOMFFUkv7mj7Rz-q-Qi-g9pehAwvVV3WKY3bxWX99KWGqVINnP8UBP7K0lMX9qr5QtJG-n6r28hRTEU6GjjzKYxfGhW6-g1_LibWy-J4QXSqAW8sRooYrhTrTfQeKa9EucgHO3390G4xI4et_KIJT1rChvmPh5_c_eK69EJeQ36KDMsoO2le9SjRh0ed_9hd55f3xzix3G0H-8sZMFkhbLlGATuJvRVpWwbOSUhBTMD8CBTfCNegFSY894bR9rSFyEYEf9EeYCPQh5ICccrX4vxOKiloBvF1wcXcdOyhYe62MBRKgGDSs01tg9dY1gMJy6rEIAvVb-qEW1mvkUoUPFSh-bUoIkOA9rRzSmpbyAWcyEZbCr4aKJHQftDDdTHNHJFNFbO6ipWsk8Jt_PRIjwG_sMun-3cWFTx4V0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to request scores for all articles based on revision ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a Function to get the Quality Score (Prediction) for each article based on its current Revision ID. ORES API is used to extract scores by passing each article in the function using the apply function and then getting the scores out as a dictionary for evry  Revision ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition\n",
    "def get_ores(df):\n",
    "    try:\n",
    "        score = request_ores_score_per_article(article_revid=df['lastrevid'],\n",
    "                                                   email_address=\"nshah23@uw.edu\",\n",
    "                                                   access_token=ACCESS_TOKEN)\n",
    "        ores_dict[df['lastrevid']]=score['enwiki']['scores'][str(df['lastrevid'])]['articlequality']['score']\n",
    "    except:\n",
    "        noscore_list.append(df['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function and passing each row: article title and revision ID to get the corresponding score dictionary. Score dictionary willbe generated for every article and is appended to an empty dictionary. Thus a final dictionary will be generated with the prediction and score corresponding to revision ID key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call\n",
    "ores_dict = dict()\n",
    "noscore_list = []\n",
    "y = page_info_df_final.apply(get_ores, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dictionary into a dataframe and ahnging column names based on further requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ores_df = pd.DataFrame(ores_dict).T\n",
    "ores_df = ores_df.reset_index()\n",
    "ores_df.rename(columns={'index':'lastrevid'},inplace=True)\n",
    "ores_df = ores_df[['lastrevid','prediction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the dataframes and merging them for the final analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info_df = page_info_df[['title','lastrevid']]\n",
    "\n",
    "# Merging the wikipedia and ores df on lastrevid\n",
    "wiki_ores_df = pd.merge(page_info_df,ores_df,on=\"lastrevid\",how=\"inner\")\n",
    "\n",
    "us_cities_df = pd.read_csv('us_cities_by_state_SEPT.2023.csv')\n",
    "\n",
    "# Merging us_cities_by_state_SEPT.2023 df with wiki_ores_df on article title to get the revision ID.\n",
    "wiki_final = pd.merge(us_cities_df,wiki_ores_df,left_on='page_title',right_on='title',how=\"left\")\n",
    "\n",
    "#Get Population df - Estimated population 2022\n",
    "population_df = pd.read_csv('NST-EST2022-ALLDATA.csv')\n",
    "population_df = population_df[['NAME','POPESTIMATE2022']]\n",
    "\n",
    "# Create all merge df after merging all data frames.\n",
    "all_merged_df = pd.merge(wiki_final,population_df,left_on=\"state\",right_on=\"NAME\",how=\"left\")\n",
    "# Replace \"_\" and \" (U.S. state)\" as they are extra credentials applied to some states based on population data and had extra \"_\". \n",
    "all_merged_df['state'] = all_merged_df['state'].str.replace(\"_\",\" \")\n",
    "all_merged_df['state'] = all_merged_df['state'].str.replace(\" (U.S. state)\",\"\")\n",
    "\n",
    "# Add Region and Division\n",
    "region_division_df = pd.read_excel('US States by Region - US Census Bureau.xlsx',index_col=[0,1])\n",
    "region_division_df = region_division_df.droplevel(0).dropna().reset_index()\n",
    "region_division_df.columns=['regional_division','state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all the datasets into a final dataframe. Creating the final dataframe for this assignment which will be used for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Dataframe\n",
    "all_merged_df_division = pd.merge(all_merged_df,region_division_df,on=\"state\",how=\"inner\")\n",
    "all_merged_df_division.drop(columns=['page_title','NAME','url'],inplace=True)\n",
    "all_merged_df_division = all_merged_df_division.rename(columns={'prediction':'article_quality','POPESTIMATE2022':'population','title':'article_title','lastrevid':'revision_id'})\n",
    "wp_scored_city_articles_by_state = all_merged_df_division[['state','regional_division','population','article_title','revision_id','article_quality']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final  df as 'wp_scored_city_articles_by_state.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_scored_city_articles_by_state.to_csv('wp_scored_city_articles_by_state.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis consists of calculating total-articles-per-population (a ratio representing the number of articles per person)  and high-quality-articles-per-population (a ratio representing the number of high-quality articles per person) on a state-by-state and divisional basis. All of these values are “per capita” ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the data by state, regional_division and population and getting total articles. Then calculating articles perrr capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = wp_scored_city_articles_by_state\n",
    "state_df = final_df.groupby(['state','regional_division','population']).agg(total_articles=('article_title','count')).reset_index()\n",
    "state_df['tot_articles_per_capita'] = state_df['total_articles']/state_df['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 US states by coverage: The 10 US states with the highest total articles per capita (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>tot_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>New England</td>\n",
       "      <td>647064.0</td>\n",
       "      <td>329</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine</td>\n",
       "      <td>New England</td>\n",
       "      <td>1385340.0</td>\n",
       "      <td>483</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>3200517.0</td>\n",
       "      <td>1041</td>\n",
       "      <td>0.000325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>733583.0</td>\n",
       "      <td>149</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>12972008.0</td>\n",
       "      <td>2549</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>918</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>10034113.0</td>\n",
       "      <td>1767</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>99</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>3045637.0</td>\n",
       "      <td>499</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>6177957.0</td>\n",
       "      <td>948</td>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          state   regional_division  population  total_articles  \\\n",
       "0       Vermont         New England    647064.0             329   \n",
       "1         Maine         New England   1385340.0             483   \n",
       "2          Iowa  West North Central   3200517.0            1041   \n",
       "3        Alaska             Pacific    733583.0             149   \n",
       "4  Pennsylvania     Middle Atlantic  12972008.0            2549   \n",
       "5       Alabama  East South Central   5074296.0             918   \n",
       "6      Michigan  East North Central  10034113.0            1767   \n",
       "7       Wyoming            Mountain    581381.0              99   \n",
       "8      Arkansas  West South Central   3045637.0             499   \n",
       "9      Missouri  West North Central   6177957.0             948   \n",
       "\n",
       "   tot_articles_per_capita  \n",
       "0                 0.000508  \n",
       "1                 0.000349  \n",
       "2                 0.000325  \n",
       "3                 0.000203  \n",
       "4                 0.000197  \n",
       "5                 0.000181  \n",
       "6                 0.000176  \n",
       "7                 0.000170  \n",
       "8                 0.000164  \n",
       "9                 0.000153  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10states_by_coverage = state_df.sort_values('tot_articles_per_capita',ascending=False).head(10).reset_index(drop=True)\n",
    "top10states_by_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom 10 US states by coverage: The 10 US states with the lowest total articles per capita (in ascending order) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>tot_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>3177772.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>39029342.0</td>\n",
       "      <td>475</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>7359197.0</td>\n",
       "      <td>91</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>4019800.0</td>\n",
       "      <td>74</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Florida</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>22244823.0</td>\n",
       "      <td>410</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>2937150.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>6164660.0</td>\n",
       "      <td>156</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>8683619.0</td>\n",
       "      <td>264</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>5892539.0</td>\n",
       "      <td>191</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>7785786.0</td>\n",
       "      <td>280</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state   regional_division  population  total_articles  \\\n",
       "0      Nevada            Mountain   3177772.0              19   \n",
       "1  California             Pacific  39029342.0             475   \n",
       "2     Arizona            Mountain   7359197.0              91   \n",
       "3    Oklahoma  West South Central   4019800.0              74   \n",
       "4     Florida      South Atlantic  22244823.0             410   \n",
       "5      Kansas  West North Central   2937150.0              61   \n",
       "6    Maryland      South Atlantic   6164660.0             156   \n",
       "7    Virginia      South Atlantic   8683619.0             264   \n",
       "8   Wisconsin  East North Central   5892539.0             191   \n",
       "9  Washington             Pacific   7785786.0             280   \n",
       "\n",
       "   tot_articles_per_capita  \n",
       "0                 0.000006  \n",
       "1                 0.000012  \n",
       "2                 0.000012  \n",
       "3                 0.000018  \n",
       "4                 0.000018  \n",
       "5                 0.000021  \n",
       "6                 0.000025  \n",
       "7                 0.000030  \n",
       "8                 0.000032  \n",
       "9                 0.000036  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom10states_by_coverage = state_df.sort_values('tot_articles_per_capita').head(10).reset_index(drop=True)\n",
    "bottom10states_by_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the data with a high-quality filter i.e. article_quality is \"FA\" OR \"GA\". Grouping the data then generated by state, regional_division, and population and getting total articles. Then calculating articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_quality_list = [\"FA\",\"GA\"]\n",
    "high_quality_df = final_df[final_df['article_quality'].isin(high_quality_list)]\n",
    "quality_df = high_quality_df.groupby(['state','regional_division','population']).agg(total_articles=('article_title','count')).reset_index()\n",
    "quality_df['tot_articles_per_capita'] = quality_df['total_articles']/quality_df['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 US states by high quality: The 10 US states with the highest high quality articles per capita (in descending order) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>tot_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>New England</td>\n",
       "      <td>647064.0</td>\n",
       "      <td>45</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Montana</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>1122867.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>12972008.0</td>\n",
       "      <td>563</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>6177957.0</td>\n",
       "      <td>262</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>733583.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>3200517.0</td>\n",
       "      <td>104</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>4240137.0</td>\n",
       "      <td>134</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maine</td>\n",
       "      <td>New England</td>\n",
       "      <td>1385340.0</td>\n",
       "      <td>43</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>5717184.0</td>\n",
       "      <td>167</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          state   regional_division  population  total_articles  \\\n",
       "0       Vermont         New England    647064.0              45   \n",
       "1       Wyoming            Mountain    581381.0              39   \n",
       "2       Montana            Mountain   1122867.0              55   \n",
       "3  Pennsylvania     Middle Atlantic  12972008.0             563   \n",
       "4      Missouri  West North Central   6177957.0             262   \n",
       "5        Alaska             Pacific    733583.0              31   \n",
       "6          Iowa  West North Central   3200517.0             104   \n",
       "7        Oregon             Pacific   4240137.0             134   \n",
       "8         Maine         New England   1385340.0              43   \n",
       "9     Minnesota  West North Central   5717184.0             167   \n",
       "\n",
       "   tot_articles_per_capita  \n",
       "0                 0.000070  \n",
       "1                 0.000067  \n",
       "2                 0.000049  \n",
       "3                 0.000043  \n",
       "4                 0.000042  \n",
       "5                 0.000042  \n",
       "6                 0.000032  \n",
       "7                 0.000032  \n",
       "8                 0.000031  \n",
       "9                 0.000029  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10states_by_quality = quality_df.sort_values('tot_articles_per_capita',ascending=False).head(10).reset_index(drop=True)\n",
    "top10states_by_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom 10 US states by high quality: The 10 US states with the lowest high quality articles per capita (in ascending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>tot_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>3177772.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>7359197.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>8683619.0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>39029342.0</td>\n",
       "      <td>171</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Florida</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>22244823.0</td>\n",
       "      <td>119</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>2937150.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>6164660.0</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>4019800.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>New England</td>\n",
       "      <td>6981974.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>4590241.0</td>\n",
       "      <td>44</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state   regional_division  population  total_articles  \\\n",
       "0         Nevada            Mountain   3177772.0               8   \n",
       "1        Arizona            Mountain   7359197.0              24   \n",
       "2       Virginia      South Atlantic   8683619.0              36   \n",
       "3     California             Pacific  39029342.0             171   \n",
       "4        Florida      South Atlantic  22244823.0             119   \n",
       "5         Kansas  West North Central   2937150.0              20   \n",
       "6       Maryland      South Atlantic   6164660.0              42   \n",
       "7       Oklahoma  West South Central   4019800.0              31   \n",
       "8  Massachusetts         New England   6981974.0              61   \n",
       "9      Louisiana  West South Central   4590241.0              44   \n",
       "\n",
       "   tot_articles_per_capita  \n",
       "0                 0.000003  \n",
       "1                 0.000003  \n",
       "2                 0.000004  \n",
       "3                 0.000004  \n",
       "4                 0.000005  \n",
       "5                 0.000007  \n",
       "6                 0.000007  \n",
       "7                 0.000008  \n",
       "8                 0.000009  \n",
       "9                 0.000010  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom10states_by_quality = quality_df.sort_values('tot_articles_per_capita').head(10).reset_index(drop=True)\n",
    "bottom10states_by_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating total articles by coverage and quality based on division. Calculating total articles on top of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census divisions by total coverage: A rank ordered list of US census divisions (in descending order) by total articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>tot_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New England</td>\n",
       "      <td>1427</td>\n",
       "      <td>3.339658e+09</td>\n",
       "      <td>4.272892e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>1208</td>\n",
       "      <td>4.005255e+09</td>\n",
       "      <td>3.016038e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>3566</td>\n",
       "      <td>1.428089e+10</td>\n",
       "      <td>2.497043e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>1984</td>\n",
       "      <td>9.921118e+09</td>\n",
       "      <td>1.999775e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>1933</td>\n",
       "      <td>1.252285e+10</td>\n",
       "      <td>1.543578e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>3773</td>\n",
       "      <td>3.315645e+10</td>\n",
       "      <td>1.137938e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>4726</td>\n",
       "      <td>5.000600e+10</td>\n",
       "      <td>9.450867e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>1284</td>\n",
       "      <td>2.234860e+10</td>\n",
       "      <td>5.745328e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>2094</td>\n",
       "      <td>4.006602e+10</td>\n",
       "      <td>5.226374e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  total_articles    population  tot_articles_per_capita\n",
       "0         New England            1427  3.339658e+09             4.272892e-07\n",
       "1            Mountain            1208  4.005255e+09             3.016038e-07\n",
       "2  West North Central            3566  1.428089e+10             2.497043e-07\n",
       "3  East South Central            1984  9.921118e+09             1.999775e-07\n",
       "4      South Atlantic            1933  1.252285e+10             1.543578e-07\n",
       "5     Middle Atlantic            3773  3.315645e+10             1.137938e-07\n",
       "6  East North Central            4726  5.000600e+10             9.450867e-08\n",
       "7             Pacific            1284  2.234860e+10             5.745328e-08\n",
       "8  West South Central            2094  4.006602e+10             5.226374e-08"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "division_df = final_df.groupby('regional_division').agg(total_articles=('article_title','count'),population=('population','sum')).reset_index()\n",
    "division_df['tot_articles_per_capita'] = division_df['total_articles']/division_df['population']\n",
    "census_divisions_by_total_coverage = division_df.sort_values('tot_articles_per_capita',ascending=False).reset_index(drop=True)\n",
    "census_divisions_by_total_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census divisions by high quality coverage: Rank ordered list of US census divisions (in descending order) by high quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>tot_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New England</td>\n",
       "      <td>224</td>\n",
       "      <td>5.145879e+08</td>\n",
       "      <td>4.352998e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>341</td>\n",
       "      <td>1.051078e+09</td>\n",
       "      <td>3.244290e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>635</td>\n",
       "      <td>2.964991e+09</td>\n",
       "      <td>2.141659e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>369</td>\n",
       "      <td>2.038105e+09</td>\n",
       "      <td>1.810506e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>514</td>\n",
       "      <td>3.244120e+09</td>\n",
       "      <td>1.584405e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>1047</td>\n",
       "      <td>7.303241e+09</td>\n",
       "      <td>1.433610e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>714</td>\n",
       "      <td>7.360117e+09</td>\n",
       "      <td>9.700932e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>481</td>\n",
       "      <td>8.203508e+09</td>\n",
       "      <td>5.863345e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>631</td>\n",
       "      <td>1.508018e+10</td>\n",
       "      <td>4.184299e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  total_articles    population  tot_articles_per_capita\n",
       "0         New England             224  5.145879e+08             4.352998e-07\n",
       "1            Mountain             341  1.051078e+09             3.244290e-07\n",
       "2  West North Central             635  2.964991e+09             2.141659e-07\n",
       "3  East South Central             369  2.038105e+09             1.810506e-07\n",
       "4      South Atlantic             514  3.244120e+09             1.584405e-07\n",
       "5     Middle Atlantic            1047  7.303241e+09             1.433610e-07\n",
       "6  East North Central             714  7.360117e+09             9.700932e-08\n",
       "7             Pacific             481  8.203508e+09             5.863345e-08\n",
       "8  West South Central             631  1.508018e+10             4.184299e-08"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_quality_list = [\"FA\",\"GA\"]\n",
    "division_high_quality_df = final_df[final_df['article_quality'].isin(high_quality_list)]\n",
    "division_quality_df = division_high_quality_df.groupby('regional_division').agg(total_articles=('article_title','count'),population=('population','sum')).reset_index()\n",
    "division_quality_df['tot_articles_per_capita'] = division_quality_df['total_articles']/division_quality_df['population']\n",
    "census_divisions_by_high_quality = division_quality_df.sort_values('tot_articles_per_capita',ascending=False).reset_index(drop=True)\n",
    "census_divisions_by_high_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: I have removed allthe intermediate data outputs based on the comments on HW 1. Hope this makes the notebook  less messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
