{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HW2 - Data Ingestion and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json, time, urllib.parse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get US cities and related articles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22152</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wamsutter,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22153</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wheatland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22154</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Worland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22155</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wright,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22156</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Yoder,_Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22157 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state           page_title  \\\n",
       "0      Alabama   Abbeville, Alabama   \n",
       "1      Alabama  Adamsville, Alabama   \n",
       "2      Alabama     Addison, Alabama   \n",
       "3      Alabama       Akron, Alabama   \n",
       "4      Alabama   Alabaster, Alabama   \n",
       "...        ...                  ...   \n",
       "22152  Wyoming   Wamsutter, Wyoming   \n",
       "22153  Wyoming   Wheatland, Wyoming   \n",
       "22154  Wyoming     Worland, Wyoming   \n",
       "22155  Wyoming      Wright, Wyoming   \n",
       "22156  Wyoming       Yoder, Wyoming   \n",
       "\n",
       "                                                     url  \n",
       "0       https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "1      https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "2         https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "3           https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "4       https://en.wikipedia.org/wiki/Alabaster,_Alabama  \n",
       "...                                                  ...  \n",
       "22152   https://en.wikipedia.org/wiki/Wamsutter,_Wyoming  \n",
       "22153   https://en.wikipedia.org/wiki/Wheatland,_Wyoming  \n",
       "22154     https://en.wikipedia.org/wiki/Worland,_Wyoming  \n",
       "22155      https://en.wikipedia.org/wiki/Wright,_Wyoming  \n",
       "22156       https://en.wikipedia.org/wiki/Yoder,_Wyoming  \n",
       "\n",
       "[22157 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_wiki_df = pd.read_csv('us_cities_by_state_SEPT.2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Info Extraction using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = list(cities_wiki_df['page_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to get page info for each article title. We will pass article title to this function and it will store the information related to the article including the revision ID into a dictionary for all 22,157 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition\n",
    "def get_page_info(df):\n",
    "    article_title = df['page_title']\n",
    "    try:\n",
    "        info = request_pageinfo_per_article(article_title)\n",
    "        page_dict = info['query']['pages']\n",
    "        all_page_info_dict.update(page_dict)\n",
    "    except:\n",
    "        print(article_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "22152    None\n",
       "22153    None\n",
       "22154    None\n",
       "22155    None\n",
       "22156    None\n",
       "Length: 22157, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function Call\n",
    "all_page_info_dict = dict()\n",
    "cities_wiki_df.apply(get_page_info,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info_df = pd.DataFrame(all_page_info_dict).T.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info_df.to_csv('page_info_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info_df = pd.read_csv('page_info_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>lastrevid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104730</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104761</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105188</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104726</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105109</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21514</th>\n",
       "      <td>140221</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>1169591845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21515</th>\n",
       "      <td>140185</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>1176370621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21516</th>\n",
       "      <td>140245</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>1166347917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21517</th>\n",
       "      <td>140070</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>1166334449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21518</th>\n",
       "      <td>140112</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>1171182284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21519 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pageid                title   lastrevid\n",
       "0      104730   Abbeville, Alabama  1171163550\n",
       "1      104761  Adamsville, Alabama  1177621427\n",
       "2      105188     Addison, Alabama  1168359898\n",
       "3      104726       Akron, Alabama  1165909508\n",
       "4      105109   Alabaster, Alabama  1179139816\n",
       "...       ...                  ...         ...\n",
       "21514  140221   Wamsutter, Wyoming  1169591845\n",
       "21515  140185   Wheatland, Wyoming  1176370621\n",
       "21516  140245     Worland, Wyoming  1166347917\n",
       "21517  140070      Wright, Wyoming  1166334449\n",
       "21518  140112       Yoder, Wyoming  1171182284\n",
       "\n",
       "[21519 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_info_df_final = page_info_df[['pageid','title','lastrevid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORES Score extraction using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<nshah23@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \"Nshah23\"\n",
    "ACCESS_TOKEN = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIwMjExZWE0YzgxYjI1OTBlYjAyMzE4MzkwOGQ5NDRhYSIsImp0aSI6IjA0YjIxZGZiMjkzZWVjYmZkNmUxNmY2OWZkMTQ5YjYzODRiMzdmNTRiYzJmNTNlMWQxN2IwYzFhZDkzYzQwNjE0OGJiNTc4MmZiM2VlNmQ2IiwiaWF0IjoxNjk3NDk5MzY4LjI0NzM0NywibmJmIjoxNjk3NDk5MzY4LjI0NzM1LCJleHAiOjMzMjU0NDA4MTY4LjI0NDc3NCwic3ViIjoiNzQwMjE2MTIiLCJpc3MiOiJodHRwczovL21ldGEud2lraW1lZGlhLm9yZyIsInJhdGVsaW1pdCI6eyJyZXF1ZXN0c19wZXJfdW5pdCI6NTAwMCwidW5pdCI6IkhPVVIifSwic2NvcGVzIjpbImJhc2ljIl19.CB6-OrWFJZ7DMAyv_Iktz7UKL_e2Wz_ZeFY6lZQUREpgntObptjYq-GKgKVfKXfEE19HZTg_RCCe4rbQ5mFDcCP9Jg5TFXQWOE8ot5QCi4dnOPjlV0DCWNmJQQRU7ulwvl1Bqzmt33REU9n9FszvZ65vaRKfHq_leB7SB-Yldr9qUfsRqJ3nUDLPXer6NaFwR_YUPP-9gh225SgMgjW-_6n-vDYBtrw-3WL4PcXbZImcMA-J0_0QNUA-LyfwqodmCSIXexOMFFUkv7mj7Rz-q-Qi-g9pehAwvVV3WKY3bxWX99KWGqVINnP8UBP7K0lMX9qr5QtJG-n6r28hRTEU6GjjzKYxfGhW6-g1_LibWy-J4QXSqAW8sRooYrhTrTfQeKa9EucgHO3390G4xI4et_KIJT1rChvmPh5_c_eK69EJeQ36KDMsoO2le9SjRh0ed_9hd55f3xzix3G0H-8sZMFkhbLlGATuJvRVpWwbOSUhBTMD8CBTfCNegFSY894bR9rSFyEYEf9EeYCPQh5ICccrX4vxOKiloBvF1wcXcdOyhYe62MBRKgGDSs01tg9dY1gMJy6rEIAvVb-qEW1mvkUoUPFSh-bUoIkOA9rRzSmpbyAWcyEZbCr4aKJHQftDDdTHNHJFNFbO6ipWsk8Jt_PRIjwG_sMun-3cWFTx4V0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding the ACCESS_TOKEN:\n",
      "Token Header: {\n",
      "    \"typ\": \"JWT\",\n",
      "    \"alg\": \"RS256\"\n",
      "}\n",
      "Token Payload: {\n",
      "    \"aud\": \"0211ea4c81b2590eb023183908d944aa\",\n",
      "    \"jti\": \"04b21dfb293eecbfd6e16f69fd149b6384b37f54bc2f53e1d17b0c1ad93c406148bb5782fb3ee6d6\",\n",
      "    \"iat\": 1697499368.247347,\n",
      "    \"nbf\": 1697499368.24735,\n",
      "    \"exp\": 33254408168.244774,\n",
      "    \"sub\": \"74021612\",\n",
      "    \"iss\": \"https://meta.wikimedia.org\",\n",
      "    \"ratelimit\": {\n",
      "        \"requests_per_unit\": 5000,\n",
      "        \"unit\": \"HOUR\"\n",
      "    },\n",
      "    \"scopes\": [\n",
      "        \"basic\"\n",
      "    ]\n",
      "}\n",
      "Token Signature: <value_suppressed>\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#   Decode the Wikimedia JWT Access token\n",
    "#\n",
    "#   NOTE: This is not required to use LiftWing to request ORES scores. This is just being done to satisfy my curiosity.\n",
    "#\n",
    "import base64\n",
    "\n",
    "print(\"Decoding the ACCESS_TOKEN:\")\n",
    "try:\n",
    "    token_components = ACCESS_TOKEN.split(\".\")\n",
    "    if len(token_components) == 3:\n",
    "        header = json.loads(base64.b64decode(token_components[0]).decode())\n",
    "        payload = json.loads(base64.b64decode(token_components[1]).decode())\n",
    "        print(\"Token Header:\",json.dumps(header,indent=4))\n",
    "        print(\"Token Payload:\",json.dumps(payload,indent=4))\n",
    "        #print(\"Token Signature:\",token_components[2])\n",
    "        print(\"Token Signature: <value_suppressed>\")\n",
    "        #\n",
    "        #  One should be able to use public/private keys to actually validate that signature - left as an exercise for later\n",
    "        #\n",
    "    else:\n",
    "        print(f\"The ACCESS_TOKEN appears to be improperly structured. It should have 3 components and it has {len(token_components)}\")\n",
    "except Exception as ex:\n",
    "    print(f\"Looks like the ACCESS_TOKEN is undefined or an empty value\")\n",
    "    raise(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a Function to get the Quality Score (Prediction) for each article based on its current Revision ID. ORES API is used to extract scores by passing each article in the function using the apply function and then getting the scores out as a dictionary for evry  Revision ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition\n",
    "def get_ores(df):\n",
    "    try:\n",
    "        score = request_ores_score_per_article(article_revid=df['lastrevid'],\n",
    "                                               email_address=\"nshah23@uw.edu\",\n",
    "                                               access_token=ACCESS_TOKEN)\n",
    "        # print(score)\n",
    "        # score_dict = score['scores']\n",
    "        ores_dict[df['lastrevid']]=score['enwiki']['scores'][str(df['lastrevid'])]['articlequality']['score']\n",
    "    except:\n",
    "        noscore_list.append(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call\n",
    "ores_dict = dict()\n",
    "noscore_list = []\n",
    "tqdm.pandas()\n",
    "page_info_df_final.progress_apply(get_ores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ores_df = pd.DataFrame(ores_dict).T\n",
    "ores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA MERGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the dataframes and merging them for the final analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info_df = pd.read_csv('page_info_df.csv')\n",
    "page_info_df = page_info_df[['title','lastrevid']]\n",
    "\n",
    "ores_df = pd.read_csv('request_ores_score_per_article_output.csv')\n",
    "\n",
    "wiki_ores_df = pd.merge(page_info_df,ores_df,on=\"lastrevid\",how=\"inner\")\n",
    "\n",
    "us_cities_df = pd.read_csv('us_cities_by_state_SEPT.2023.csv')\n",
    "\n",
    "wiki_final = pd.merge(us_cities_df,wiki_ores_df,left_on='page_title',right_on='title',how=\"left\")\n",
    "\n",
    "population_df = pd.read_csv('NST-EST2022-ALLDATA.csv')\n",
    "population_df = population_df[['NAME','POPESTIMATE2022']]\n",
    "\n",
    "all_merged_df = pd.merge(wiki_final,population_df,left_on=\"state\",right_on=\"NAME\",how=\"left\")\n",
    "all_merged_df['state'] = all_merged_df['state'].str.replace(\"_\",\" \")\n",
    "all_merged_df['state'] = all_merged_df['state'].str.replace(\" (U.S. state)\",\"\")\n",
    "\n",
    "region_division_df = pd.read_excel('US States by Region - US Census Bureau.xlsx',index_col=[0,1])\n",
    "region_division_df = region_division_df.droplevel(0).dropna().reset_index()\n",
    "region_division_df.columns=['regional_division','state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all the datasets into a final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged_df_division = pd.merge(all_merged_df,region_division_df,on=\"state\",how=\"inner\")\n",
    "all_merged_df_division.drop(columns=['page_title','NAME','url'],inplace=True)\n",
    "all_merged_df_division = all_merged_df_division.rename(columns={'prediction':'article_quality','POPESTIMATE2022':'population','title':'article_title','lastrevid':'revision_id'})\n",
    "wp_scored_city_articles_by_state = all_merged_df_division[['state','regional_division','population','article_title','revision_id','article_quality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_scored_city_articles_by_state.to_csv('wp_scored_city_articles_by_state.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = wp_scored_city_articles_by_state\n",
    "state_df = final_df.groupby(['state','regional_division','population']).agg(total_articles=('article_title','count')).reset_index()\n",
    "state_df['tot_articles_per_capita'] = state_df['total_articles']/state_df['population']\n",
    "state_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 US states by coverage: The 10 US states with the highest total articles per capita (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10states_by_coverage = state_df.sort_values('tot_articles_per_capita',ascending=False).head(10).reset_index(drop=True)\n",
    "top10states_by_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom 10 US states by coverage: The 10 US states with the lowest total articles per capita (in ascending order) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom10states_by_coverage = state_df.sort_values('tot_articles_per_capita').head(10).reset_index(drop=True)\n",
    "bottom10states_by_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_quality_list = [\"FA\",\"GA\"]\n",
    "high_quality_df = final_df[final_df['article_quality'].isin(high_quality_list)]\n",
    "quality_df = high_quality_df.groupby(['state','regional_division','population']).agg(total_articles=('article_title','count')).reset_index()\n",
    "quality_df['tot_articles_per_capita'] = quality_df['total_articles']/quality_df['population']\n",
    "quality_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 US states by high quality: The 10 US states with the highest high quality articles per capita (in descending order) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10states_by_quality = quality_df.sort_values('tot_articles_per_capita',ascending=False).head(10).reset_index(drop=True)\n",
    "top10states_by_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom 10 US states by high quality: The 10 US states with the lowest high quality articles per capita (in ascending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom10states_by_quality = quality_df.sort_values('tot_articles_per_capita').head(10).reset_index(drop=True)\n",
    "bottom10states_by_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census divisions by total coverage: A rank ordered list of US census divisions (in descending order) by total articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "division_df = final_df.groupby('regional_division').agg(total_articles=('article_title','count'),population=('population','sum')).reset_index()\n",
    "division_df['tot_articles_per_capita'] = division_df['total_articles']/division_df['population']\n",
    "census_divisions_by_total_coverage = division_df.sort_values('tot_articles_per_capita',ascending=False).reset_index(drop=True)\n",
    "census_divisions_by_total_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census divisions by high quality coverage: Rank ordered list of US census divisions (in descending order) by high quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_quality_list = [\"FA\",\"GA\"]\n",
    "division_high_quality_df = final_df[final_df['article_quality'].isin(high_quality_list)]\n",
    "division_quality_df = division_high_quality_df.groupby('regional_division').agg(total_articles=('article_title','count'),population=('population','sum')).reset_index()\n",
    "division_quality_df['tot_articles_per_capita'] = division_quality_df['total_articles']/division_quality_df['population']\n",
    "census_divisions_by_high_quality = division_quality_df.sort_values('tot_articles_per_capita',ascending=False).reset_index(drop=True)\n",
    "census_divisions_by_high_quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
